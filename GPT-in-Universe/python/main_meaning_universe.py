# ==============================================================
# ğŸŒŒ AI Meaning Universe â€” main_meaning_universe.py
# æ—¥æœ¬èªã®è¤‡æ•°è³ªå• â†’ å›ç­”ç¾¤ç”Ÿæˆ â†’ TF-IDF â†’ PCA(3D) â†’ KMeans
# â†’ universe.jsonï¼ˆThree.jsç”¨ï¼‰ã¨CSVã‚’å‡ºåŠ›
# ä¾å­˜: openai, pandas, numpy, scikit-learn, matplotlib(ä»»æ„), tqdm
# ==============================================================
# English: Entry script for generating an "AI Meaning Universe":
# - Ask multiple Japanese philosophical questions
# - Generate multiple answers via OpenAI API
# - Vectorize with TF-IDF, reduce to 3D with PCA, cluster with KMeans
# - Export CSV for analysis and universe.json for Three.js visualization
# æ—¥æœ¬èª: ã€ŒAI Meaning Universeã€ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
# ãƒ»è¤‡æ•°ã®å“²å­¦çš„è³ªå•ã‚’æ—¥æœ¬èªã§æŠ•ã’ã‚‹
# ãƒ»OpenAI APIã§è¤‡æ•°å›ç­”ã‚’ç”Ÿæˆ
# ãƒ»TF-IDFã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–â†’PCAã§3æ¬¡å…ƒåŒ–â†’KMeansã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
# ãƒ»è§£æç”¨CSVã¨Three.jså¯è¦–åŒ–ç”¨universe.jsonã‚’å‡ºåŠ›

import os
import re
import json
from dataclasses import dataclass
from typing import List, Tuple

import numpy as np
import pandas as pd
from tqdm import tqdm

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# --- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆå¯¾ç­–ï¼ˆä»»æ„ï¼‰ ---
# English: Optional Japanese font setup for matplotlib (useful for local debugging/plots).
# æ—¥æœ¬èª: è§£ææ™‚ã®æ—¥æœ¬èªè¡¨ç¤ºå´©ã‚Œå¯¾ç­–ï¼ˆmatplotlibã§ã®ãƒ‡ãƒãƒƒã‚°å¯è¦–åŒ–å‘ã‘ã€ä»»æ„ï¼‰ã€‚
try:
    import matplotlib.pyplot as plt  # è§£æãƒ‡ãƒãƒƒã‚°ã§ä½¿ã†ãªã‚‰
    import matplotlib.font_manager as fm
    if os.name == "nt":
        FONT_PATH = "C:/Windows/Fonts/msgothic.ttc"  # å¿…è¦ã«å¿œã˜ã¦å¤‰æ›´
        if os.path.exists(FONT_PATH):
            plt.rcParams["font.family"] = fm.FontProperties(fname=FONT_PATH).get_name()
            plt.rcParams["axes.unicode_minus"] = False
    try:
        import japanize_matplotlib  # noqa: F401
    except Exception:
        pass
except Exception:
    pass

# --- OpenAI SDK ---
# English: Initialize OpenAI client. Prefer setting OPENAI_API_KEY via environment variable.
# æ—¥æœ¬èª: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã€‚APIã‚­ãƒ¼ã¯ç’°å¢ƒå¤‰æ•°OPENAI_API_KEYã§è¨­å®šæ¨å¥¨ã€‚
from openai import OpenAI
client = OpenAI(api_key="")  # ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒå¿…è¦

# --- JSONã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–¢æ•° ---
# åŒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¾ãŸã¯ãƒ‘ã‚¹ã‚’é€šã—ãŸä¸Šã§èª­ã¿è¾¼ã¿
# English: Export helper for Three.js universe.json; must exist in import path.
# æ—¥æœ¬èª: Three.jså‘ã‘universe.jsonã‚’æ›¸ãå‡ºã™è£œåŠ©é–¢æ•°ã€‚importãƒ‘ã‚¹ä¸Šã«é…ç½®ã—ã¦ãŠãã“ã¨ã€‚
from exporter_universe_json import export_universe_json


# ==========================
# è¨­å®š
# ==========================
# English: Model name used for chat completions.
# æ—¥æœ¬èª: å›ç­”ç”Ÿæˆã«ç”¨ã„ã‚‹ãƒ¢ãƒ‡ãƒ«åã€‚
MODEL = "gpt-4o"

# â˜…ã“ã“ã‚’ç·¨é›†ï¼šè³ªå•ãƒªã‚¹ãƒˆï¼ã‚µãƒ³ãƒ—ãƒ«æ•°
# English: List of prompts (questions). Keep them concise; model returns short answers (<=30 chars).
# æ—¥æœ¬èª: è³ªå•æ–‡ã®ä¸€è¦§ã€‚çŸ­æ–‡å›ç­”ï¼ˆ30æ–‡å­—ä»¥å†…ï¼‰ã‚’æƒ³å®šã€‚
QUESTIONS: List[str] = [
    "äººé–“ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "é­‚ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "AIã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "ç”Ÿå‘½ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "æ„è­˜ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "æ™‚é–“ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚",
    "å®‡å®™ã¨ã¯ä½•ã‹ï¼Ÿ30æ–‡å­—ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚"
]

# English: Number of samples per question; larger values give denser clouds but take more tokens/time.
# æ—¥æœ¬èª: è³ªå•ã”ã¨ã®ç”Ÿæˆå›æ•°ã€‚å¤§ããã™ã‚‹ã¨åˆ†å¸ƒãŒå®‰å®šã™ã‚‹ãŒã‚³ã‚¹ãƒˆ/æ™‚é–“å¢—ã€‚
NUM_SAMPLES_PER_QUESTION = 15   # ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ãªã‚‰ 20ã€œ50 æ¨å¥¨
# English: Number of clusters. If None, it is estimated from point count (see auto_k).
# æ—¥æœ¬èª: ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã€‚Noneãªã‚‰ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã‹ã‚‰è‡ªå‹•æ¨å®šï¼ˆauto_kå‚ç…§ï¼‰ã€‚
NUM_CLUSTERS = None             # è‡ªå‹•æ¨å®šï¼ˆå›ºå®šãªã‚‰æ•´æ•°ï¼‰

# å‡ºåŠ›å…ˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰è¦‹ãŸç›¸å¯¾ãƒ‘ã‚¹ã‚’æƒ³å®šï¼‰
# English: Output paths (relative to project root). CSV for raw dataset; JSON for Three.js.
# æ—¥æœ¬èª: å‡ºåŠ›å…ˆã€‚CSVã¯ç”Ÿãƒ‡ãƒ¼ã‚¿ã€JSONã¯Three.jså¯è¦–åŒ–ç”¨ã€‚
DATA_DIR = os.path.join(os.path.dirname(__file__), "..", "data")
CSV_OUT = os.path.join(DATA_DIR, "ai_universe_dataset.csv")
JSON_OUT = os.path.join(DATA_DIR, "universe.json")

# ä¹±æ•°å›ºå®šï¼ˆå†ç¾æ€§ï¼‰
# English: Random seed for reproducibility across PCA/KMeans.
# æ—¥æœ¬èª: PCA/KMeansç­‰ã®å†ç¾æ€§ã‚’ä¿ã¤ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã€‚
RANDOM_STATE = 42


# ==========================
# ãƒ‡ãƒ¼ã‚¿åé›†
# ==========================
# English: Generate multiple answers for each question via OpenAI Chat Completions.
# - Returns a DataFrame with columns: QID, Question, Output
# æ—¥æœ¬èª: å„è³ªå•ã«å¯¾ã—ã¦OpenAIã®Chat Completionsã§è¤‡æ•°å›ç­”ã‚’ç”Ÿæˆã€‚
# ãƒ»è¿”å€¤ã¯QID/Question/Outputåˆ—ã‚’æŒã¤DataFrameã€‚
def generate_answers(questions: List[str], n_per_q: int) -> pd.DataFrame:
    rows = []
    for qi, q in enumerate(questions):
        print(f"ğŸ”¹ å•ã„ {qi+1}/{len(questions)}: '{q}' ã‚’ {n_per_q} å›ç”Ÿæˆâ€¦")
        for _ in tqdm(range(n_per_q)):
            res = client.chat.completions.create(
                model=MODEL,
                messages=[{"role": "user", "content": q}],
            )
            # English: Normalize newline to space and strip; store as one-line text.
            # æ—¥æœ¬èª: æ”¹è¡Œã‚’ç©ºç™½ã«å¤‰æ›ã—stripã€1è¡Œãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿æŒã€‚
            text = res.choices[0].message.content.strip().replace("\n", " ")
            rows.append({"QID": qi, "Question": q, "Output": text})
    df = pd.DataFrame(rows)
    return df


# ==========================
# å‰å‡¦ç†ãƒ»ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ»æ¬¡å…ƒå‰Šæ¸›ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿
# ==========================
# English: TF-IDF vectorization -> PCA to 3D.
# - Returns (fitted vectorizer, sparse matrix X, 3D numpy array pts3)
# æ—¥æœ¬èª: TF-IDFã§ãƒ™ã‚¯ãƒˆãƒ«åŒ–â†’PCAã§3æ¬¡å…ƒåŒ–ã€‚
# ãƒ»(å­¦ç¿’æ¸ˆã¿vectorizer, ç–è¡Œåˆ—X, 3æ¬¡å…ƒåº§æ¨™pts3)ã‚’è¿”ã™ã€‚
def vectorize_and_reduce(outputs: pd.Series) -> Tuple[TfidfVectorizer, np.ndarray, np.ndarray]:
    """TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã€PCAã§3æ¬¡å…ƒã«é‚„å…ƒã€‚"""
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(outputs)
    pca = PCA(n_components=3, random_state=RANDOM_STATE)
    # English: Convert sparse TF-IDF to dense before PCA.
    # æ—¥æœ¬èª: PCAé©ç”¨ã®ãŸã‚ç–è¡Œåˆ—ã‚’denseã«å¤‰æ›ã€‚
    pts3 = pca.fit_transform(X.toarray())
    return vectorizer, X, pts3


# English: Heuristic to estimate number of clusters from point count.
# æ—¥æœ¬èª: ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°ã‹ã‚‰ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¦‚ç®—ã™ã‚‹ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã€‚
def auto_k(n_points: int) -> int:
    """ç‚¹æ•°ã‹ã‚‰ãƒ©ãƒ•ã«ã‚¯ãƒ©ã‚¹ã‚¿æ•°ã‚’æ¨å®šã€‚"""
    # ã–ã£ãã‚Šï¼š15ç‚¹ã«ã¤ã1ã‚¯ãƒ©ã‚¹ã‚¿ã€6ã€œ16ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—
    # English: ~1 cluster per 15 points, clamped to [6, 16].
    # æ—¥æœ¬èª: ãŠã‚ˆã15ç‚¹ã«ã¤ã1ã‚¯ãƒ©ã‚¹ã‚¿ã€[6,16]ã«ã‚¯ãƒªãƒƒãƒ—ã€‚
    k = int(np.clip(np.round(n_points / 15), 6, 16))
    return k


# English: Run KMeans on X. If num_clusters is None, infer via auto_k.
# - Returns (fitted kmeans, labels array)
# æ—¥æœ¬èª: KMeansã«ã‚ˆã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã€‚num_clustersãŒNoneãªã‚‰auto_kã§æ¨å®šã€‚
# ãƒ»(å­¦ç¿’æ¸ˆã¿kmeans, ãƒ©ãƒ™ãƒ«é…åˆ—)ã‚’è¿”ã™ã€‚
def cluster_points(X, num_clusters: int | None) -> Tuple[KMeans, np.ndarray]:
    if num_clusters is None:
        k = auto_k(X.shape[0])
    else:
        k = int(num_clusters)
    # English: n_init="auto" (scikit-learn â‰¥1.4) for robust initialization.
    # æ—¥æœ¬èª: åˆæœŸåŒ–å›æ•°ã¯"auto"ï¼ˆscikit-learn â‰¥1.4æƒ³å®šï¼‰ã§å®‰å®šæ€§ã‚’ç¢ºä¿ã€‚
    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init="auto")
    labels = kmeans.fit_predict(X)
    return kmeans, labels


# English: For each cluster center, pick top-N terms by weight; join with "ãƒ»".
# æ—¥æœ¬èª: å„ã‚¯ãƒ©ã‚¹ã‚¿ä¸­å¿ƒã®é‡ã¿ä¸Šä½èªã‚’æŠ½å‡ºã—ã€ã€Œãƒ»ã€ã§é€£çµã—ã¦è¿”ã™ã€‚
def cluster_top_terms(vectorizer: TfidfVectorizer, kmeans: KMeans, topn: int = 5) -> List[str]:
    """å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä¸Šä½èªã‚’â€˜ãƒ»â€™ã§é€£çµã—ã¦è¿”ã™ã€‚"""
    terms = np.array(vectorizer.get_feature_names_out())
    order = kmeans.cluster_centers_.argsort()[:, ::-1]
    out = []
    for i in range(order.shape[0]):
        top = terms[order[i, :topn]]
        out.append("ãƒ»".join(top))
    return out


# ==========================
# è¦ç´„ï¼ˆå…¨ä½“ï¼‰
# ==========================
# English: Summarize the entire answer set (JA/EN). This is optional for visualization
# but useful for logging/analysis. Uses the same MODEL.
# æ—¥æœ¬èª: å…¨å›ç­”é›†åˆã®è¦ç´„ï¼ˆæ—¥æœ¬èª/è‹±èªï¼‰ã€‚å¯è¦–åŒ–ã«ã¯ä¸è¦ã ãŒè¨˜éŒ²ãƒ»è§£æã«æœ‰ç”¨ã€‚
# ç”Ÿæˆã«ã¯åŒä¸€MODELã‚’ä½¿ç”¨ã€‚
def summarize_all(df: pd.DataFrame, lang: str = "ja") -> str:
    joined = "\n".join(df["Output"].tolist())

    if lang == "ja":
        prompt = f"""
æ¬¡ã®AIã®å›ç­”ç¾¤ã¯ã€è¤‡æ•°ã®å“²å­¦çš„å•ã„ï¼ˆä¾‹ï¼š{ 'ã€'.join(QUESTIONS[:3]) } â€¦ï¼‰ã«å¯¾ã™ã‚‹ã‚‚ã®ã§ã™ã€‚
ã“ã®é›†åˆã‹ã‚‰ã€AIãŒãã‚Œã‚‰ã®æ¦‚å¿µã‚’ã©ã®ã‚ˆã†ã«ç†è§£ã—ã¦ã„ã‚‹ã‹ã‚’
â‘ å…±é€šç‚¹ â‘¡ç›¸é•ç‚¹ â‘¢åˆ†å¸ƒå‚¾å‘ï¼ˆã©ã®æ–¹å‘ã«è§£é‡ˆãŒå¯„ã‚‹ã‹ï¼‰
ã®3é …ç›®ã§ã€å¹³æ˜“ãªæ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
ã€å›ç­”ç¾¤ã€‘
{joined}
"""
    else:
        prompt = f"""
These are AI-generated answers to multiple philosophical questions
(e.g., {"; ".join(QUESTIONS[:3])} â€¦).
Please summarize how the AI seems to understand these concepts in three parts:
(1) Common points, (2) Differences, and (3) Overall tendencies of interpretation.
Use simple, clear English.
[Outputs]
{joined}
"""
    res = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
    )
    return res.choices[0].message.content.strip()


# ==========================
# ãƒ¡ã‚¤ãƒ³
# ==========================
# English: Orchestrates the pipeline:
# 1) Generate data -> CSV
# 2) TF-IDF + PCA(3D)
# 3) KMeans clustering
# 4) Extract top terms per cluster
# 5) (Optional) Summarize all outputs (JA/EN) to text files
# 6) Export Three.js-ready universe.json
# æ—¥æœ¬èª: å…¨å‡¦ç†ã®å®Ÿè¡Œé †åº:
# 1) å›ç­”ç”Ÿæˆâ†’CSVä¿å­˜
# 2) TF-IDFï¼‹PCA(3D)
# 3) KMeansã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
# 4) ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã®ä»£è¡¨èªæŠ½å‡º
# 5) ï¼ˆä»»æ„ï¼‰å…¨ä½“è¦ç´„ï¼ˆJA/ENï¼‰ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«ä¿å­˜
# 6) Three.jsç”¨universe.jsonã‚’æ›¸ãå‡ºã—
def main():
    os.makedirs(DATA_DIR, exist_ok=True)

    # 1) åé›†
    # English: Generate answers and save raw dataset for reproducibility and external analysis.
    # æ—¥æœ¬èª: å›ç­”ã‚’ç”Ÿæˆã—ã€ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’CSVã«ä¿å­˜ï¼ˆå†ç¾æ€§ãƒ»å¤–éƒ¨è§£æã®ãŸã‚ï¼‰ã€‚
    df = generate_answers(QUESTIONS, NUM_SAMPLES_PER_QUESTION)
    df.to_csv(CSV_OUT, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ CSV: {CSV_OUT}")

    # 2) ãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‹PCA(3D)
    # English: Convert texts to TF-IDF vectors and reduce to 3D coordinates for visualization.
    # æ—¥æœ¬èª: ãƒ†ã‚­ã‚¹ãƒˆâ†’TF-IDFåŒ–ã—ã€å¯è¦–åŒ–ç”¨ã«3æ¬¡å…ƒåº§æ¨™ã¸æ¬¡å…ƒåœ§ç¸®ã€‚
    vectorizer, X, pts3 = vectorize_and_reduce(df["Output"])

    # 3) ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    # English: Cluster points; attach labels back to DataFrame for downstream exports.
    # æ—¥æœ¬èª: ç‚¹ç¾¤ã‚’ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã€ãƒ©ãƒ™ãƒ«ã‚’DataFrameã¸ä»˜ä¸ã€‚
    kmeans, labels = cluster_points(X, NUM_CLUSTERS)
    df["Cluster"] = labels

    # 4) ä»£è¡¨èª
    # English: Print human-readable top terms for quick sanity check of each cluster.
    # æ—¥æœ¬èª: å„ã‚¯ãƒ©ã‚¹ã‚¿ã®ä»£è¡¨èªã‚’å‡ºåŠ›ï¼ˆäººé–“ã®ç›®ã§ã®ç´ æ—©ã„æ¤œè¨¼ç”¨ï¼‰ã€‚
    top_terms = cluster_top_terms(vectorizer, kmeans, topn=5)
    print("ğŸ§  å„ã‚¯ãƒ©ã‚¹ã‚¿ä»£è¡¨èª:")
    for i, t in enumerate(top_terms):
        print(f"  C{i}: {t}")

    # 5) å…¨ä½“è¦ç´„ï¼ˆä»»æ„ï¼Three.jsã«ã¯ä¸è¦ã ãŒãƒ­ã‚°ã¨ã—ã¦ï¼‰
    # English: Generate and persist summaries (JA/EN). Fail safe: skip on any exception.
    # æ—¥æœ¬èª: æ—¥è‹±ã®è¦ç´„æ–‡ã‚’ç”Ÿæˆã—ã¦ä¿å­˜ã€‚å¤±æ•—ã—ã¦ã‚‚å‡¦ç†ç¶™ç¶šï¼ˆä¾‹å¤–ã¯æ¡ã‚Šã¤ã¶ã™ï¼‰ã€‚
    try:
        summary_ja = summarize_all(df, lang="ja")
        summary_en = summarize_all(df, lang="en")
        with open(os.path.join(DATA_DIR, "summary_ja.txt"), "w", encoding="utf-8") as f:
            f.write(summary_ja)
        with open(os.path.join(DATA_DIR, "summary_en.txt"), "w", encoding="utf-8") as f:
            f.write(summary_en)
        print("ğŸ’¾ summaries: summary_ja.txt / summary_en.txt")
    except Exception as e:
        print(f"è¦ç´„ã¯ã‚¹ã‚­ãƒƒãƒ—: {e}")

    # 6) Three.jsç”¨ã® universe.json ã‚’å‡ºåŠ›
    # English: Final export for front-end visualization. The exporter handles schema/format.
    # æ—¥æœ¬èª: æœ€çµ‚çš„ãªThree.jså¯è¦–åŒ–å‘ã‘å‡ºåŠ›ã€‚ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯exporterå´ã«å§”è­²ã€‚
    export_universe_json(
        outfile=JSON_OUT,
        points3d=pts3,
        labels=labels,
        df=df[["Question", "Output"]],
        cluster_terms=top_terms,
        questions=QUESTIONS
    )
    print(f"âœ… å®Œäº†: {JSON_OUT}")


if __name__ == "__main__":
    main()
    # English: Standard Python entry point.
    # æ—¥æœ¬èª: Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ¨™æº–ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã€‚
